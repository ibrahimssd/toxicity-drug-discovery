{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/home/isiddig/Desktop/uds/thesis/project-code\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# print current working directory\n",
    "import os\n",
    "# cd to the previous directory\n",
    "# os.chdir('..')\n",
    "# if not 'bertviz_repo' in sys.path:\n",
    "#   print('Adding bertviz_repo to sys.path')\n",
    "#   sys.path += ['bertviz_repo']\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 01:33:47.915341: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/isiddig/anaconda3/envs/toxicity/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_cuda.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from smiles_spe_tokenizer import SMILES_SPE_Tokenizer\n",
    "import datasets\n",
    "import torch\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer , DataCollatorWithPadding , AutoTokenizer, AutoConfig , AutoModelForCausalLM\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline, RobertaModel, RobertaTokenizer\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, pipeline\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import evaluate\n",
    "from bertviz_repo.bertviz import head_view\n",
    "from CollatorDatasetTrainer import CustomDataCollator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from CollatorDatasetTrainer import CustomDataCollator , ToxicTrainer\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from utils import split_stratified_into_train_val_test\n",
    "from datasets import load_metric\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "from sklearn.metrics import balanced_accuracy_score , accuracy_score , precision_score , recall_score , f1_score , roc_auc_score\n",
    "from bert_loves_chemistry.chemberta.utils.molnet_dataloader import load_molnet_dataset, write_molnet_dataset_for_chemprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CS(=O)(=O)', 'CCN', 'Cc1ccc(', 'o1)', 'c2ccc3c(c2)', 'c(', 'ncn', '3)', 'N', 'c4cc', 'c(', 'c(', 'c4)', 'Cl)', 'OC', 'c5', 'cccc', '(', 'c5', ')', 'F']\n",
      "[12, 613, 394, 264, 923, 577, 139, 549, 162, 64, 378, 139, 139, 615, 181, 165, 524, 163, 89, 524, 27, 51, 13]\n"
     ]
    }
   ],
   "source": [
    "# prepare the tokenizers\n",
    "Spe_tokenizer = SMILES_SPE_Tokenizer(vocab_file='vocabs/spe_vocab/vocab_speSMILESDB_1000.txt', spe_file= 'vocabs/spe_vocab/cleaned_smilesDB_1000.smi', tokenizer_type='SPE')\n",
    "MacFrag_tokenizer = SMILES_SPE_Tokenizer(vocab_file='vocabs/macFrag_vocab/smilesDB_vocab.smi', tokenizer_type='MacFrag')\n",
    "\n",
    "Bpe_tokenizer = SMILES_SPE_Tokenizer(vocab_file='vocabs/bpe_vocab/bpe_DBsmi_vocab_1000.txt', \n",
    "                                     tokenizer_path='models/tokenizers/bpe_DBsmi_1000.bin',\n",
    "                                     tokenizer_type='BPE')\n",
    "\n",
    "mofressor_tokenizer = SMILES_SPE_Tokenizer(vocab_file='./vocabs/morf_vocab/morf_tox21_vocab1000.txt',\n",
    "                                           tokenizer_path='models/tokenizers/morf_tox21_1000.bin',\n",
    "                                           tokenizer_type='Morfessor')\n",
    "\n",
    "# choose tokenizer \n",
    "tokenizer = Spe_tokenizer\n",
    "smi_1 = 'CS(=O)(=O)CCNCc1ccc(o1)c2ccc3c(c2)c(ncn3)Nc4ccc(c(c4)Cl)OCc5cccc(c5)F'\n",
    "encoded_input = tokenizer.encode(smi_1)\n",
    "print(tokenizer.tokenize(smi_1))\n",
    "print(encoded_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'S(=O)(=O)', 'C', 'C', 'N', 'C', 'c1ccc(', 'o1', ')', 'c2ccc3c(c2)', 'c(', 'ncn', '3)', 'N', 'c4cc', 'c(', 'c(', 'c4', ')', 'Cl)', 'O', 'C', 'c5c', 'ccc(', 'c5', ')', 'F']\n",
      "['CS(=O)(=O)CCNCc1ccc(o1)c2ccc3c(c2) ', ' c(ncn3)Nc4ccc(c(c4)Cl)OCc5cccc(c5)F']\n",
      "['CS(=O)(=O)', 'CCN', 'Cc1ccc(', 'o1)', 'c2ccc3c(c2)', 'c(', 'ncn', '3)N', 'c4ccc(', 'c(', 'c4)C', 'l)', 'OC', 'c5cccc', '(', 'c5)', 'F']\n",
      "['CS(=O)(=O)', 'CCN', 'Cc1ccc(', 'o1)', 'c2ccc3c(c2)', 'c(', 'ncn', '3)', 'N', 'c4cc', 'c(', 'c(', 'c4)', 'Cl)', 'OC', 'c5', 'cccc', '(', 'c5', ')', 'F']\n"
     ]
    }
   ],
   "source": [
    "print(mofressor_tokenizer._tokenize('CS(=O)(=O)CCNCc1ccc(o1)c2ccc3c(c2)c(ncn3)Nc4ccc(c(c4)Cl)OCc5cccc(c5)F'))\n",
    "print(MacFrag_tokenizer._tokenize('CS(=O)(=O)CCNCc1ccc(o1)c2ccc3c(c2) , c(ncn3)Nc4ccc(c(c4)Cl)OCc5cccc(c5)F'))\n",
    "print(Bpe_tokenizer._tokenize('CS(=O)(=O)CCNCc1ccc(o1)c2ccc3c(c2)c(ncn3)Nc4ccc(c(c4)Cl)OCc5cccc(c5)F'))\n",
    "print(Spe_tokenizer._tokenize('CS(=O)(=O)CCNCc1ccc(o1)c2ccc3c(c2)c(ncn3)Nc4ccc(c(c4)Cl)OCc5cccc(c5)F'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/pre_trained_models/Best_Clintox_Spe_chemBERTa were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at models/pre_trained_models/Best_Clintox_Spe_chemBERTa and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# FINE TUNING\n",
    "# Modify tokenizer to add new tokens , and extend embeddings layers\n",
    "# initialize the model\n",
    "model_name =  \"models/pre_trained_models/Best_Clintox_Spe_chemBERTa\"\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = Spe_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(944, 4) (236, 4) (296, 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cd832afa0c484a8da99adc21f8c40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/944 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97788e1f40224d42ac2d7c9754e35a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02002499f23460fbffd759b36e2cbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/944 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566e400ad9224581a3cd2063d76f1516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train dataset: 944\n",
      "length of val dataset: 236\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "clin_tox_zinc250 = \"./datasets/cleaned_smilesDB.smi\"\n",
    "clin = \"./datasets/pre_processed/clintox.smi\"\n",
    "zinc  = \"./datasets/zinc_smiles.txt\"\n",
    "clintox_frags = './datasets/mac_fragments/clintox_fragments.smi'\n",
    "# tasks, (train_df, valid_df, test_df), transformers = load_molnet_dataset(\"clintox\", tasks_wanted=None, split='random')\n",
    "# # load tox21 dataset\n",
    "# tasks, (train_df, valid_df, test_df), transformers = load_molnet_dataset(\"tox21\", tasks_wanted= ['NR-AhR'])\n",
    "clintox_df = pd.read_csv('./datasets/post_processed/clintox_processed.csv')\n",
    "tox21_df = pd.read_csv('./datasets/post_processed/tox21_processed.csv')\n",
    "# split imbalanced dataset clintox into train, valid, test\n",
    "train_df, test_df = train_test_split(clintox_df, test_size=0.2, random_state=42 , stratify=clintox_df['CT_TOX'], shuffle=True)\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['CT_TOX'], shuffle=True)\n",
    "print(train_df.shape, valid_df.shape, test_df.shape)\n",
    "# Create a dictionary with the text data\n",
    "tr_data_dict = {\"text\": train_df['smiles'].tolist() ,  #text_data\n",
    "             \"labels\":  train_df['CT_TOX'].values}        #train_df['labels'] } #labels\n",
    "val_data_dict = {\"text\": valid_df['smiles'].tolist() ,  #text_data\n",
    "                \"labels\":  valid_df['CT_TOX'].values}    #train_df['labels'] } #labels\n",
    "\n",
    "# Create a dataset using the `Dataset` class from `datasets`\n",
    "tr_dataset= Dataset.from_dict(tr_data_dict)\n",
    "val_dataset= Dataset.from_dict(val_data_dict) \n",
    "\n",
    "tr_tokenized_dataset = tr_dataset.map(lambda examples: tokenizer(examples[\"text\"], truncation=True,padding=True, max_length=512), batched=True)\n",
    "val_tokenized_dataset = val_dataset.map(lambda examples: tokenizer(examples[\"text\"], truncation=True,padding=True, max_length=512), batched=True)\n",
    "tr_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'text'])\n",
    "val_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'text'])\n",
    "# call add_decoded_tokens to add decoded tokens to the data\n",
    "tr_tokenized_dataset = tr_tokenized_dataset.map(lambda batch: add_decoded_tokens(batch= batch, tokenizer=tokenizer))\n",
    "val_tokenized_dataset = val_tokenized_dataset.map(lambda batch: add_decoded_tokens(batch= batch, tokenizer=tokenizer))\n",
    "print(\"length of train dataset:\", len(tr_tokenized_dataset))\n",
    "print(\"length of val dataset:\", len(val_tokenized_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'CC(C)[N+](C)(CCOC(=O)C1c2ccccc2Oc3c1cccc3)C(C)C', 'labels': tensor(0), 'input_ids': tensor([ 12, 236,  94, 138, 161, 401, 149, 201,  30, 145, 153, 162, 581,  13,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'decoded_tokens': '[CLS] CC(C) [N+] (C) (CC OC(=O) C1 c2ccccc2 O c3 c1cccc 3) C(C)C [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'}\n"
     ]
    }
   ],
   "source": [
    "print(tr_tokenized_dataset[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels counts  (array([0, 1]), array([1364,  112]))\n",
      "class weights  tensor([0.5411, 6.5893])\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "data_collator= DataCollatorWithPadding(tokenizer=tokenizer, padding =\"longest\", max_length=128)\n",
    "train_loader = DataLoader(tr_tokenized_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_tokenized_dataset, batch_size=16, shuffle=True)\n",
    "# load metric for binary classification for imbalanced dataset\n",
    "roc_auc_score = evaluate.load(\"roc_auc\")\n",
    "# accuracy = load_metric(\"accuracy\")\n",
    "model_name = \"clintox_spe_classifier_chemBERTa\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/classifiers/\"+model_name,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    save_steps=10000,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    seed=42,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=roc_auc_score.name,\n",
    "    )\n",
    "\n",
    "# Calculate class weights\n",
    "print(\"labels counts \" , np.unique(clintox_df[\"CT_TOX\"], return_counts=True))\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(clintox_df[\"CT_TOX\"], return_counts=True)[0], y=clintox_df[\"CT_TOX\"])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"class weights \", class_weights)\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = ToxicTrainer(\n",
    "    model=classification_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tr_tokenized_dataset,\n",
    "    eval_dataset=val_tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics= lambda predictions: compute_metrics(predictions, class_weights),\n",
    "    weights=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4567a27342694f2dbedb08114b8081f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6896, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.33}\n",
      "{'loss': 0.6568, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6337, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d435556f7344e1b876cb0f1328457e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.592046856880188, 'eval_accuracy': 0.8569678159192854, 'eval_precision': 0.8754064674119615, 'eval_recall': 0.8333333333333334, 'eval_f1': 0.8568942986915573, 'eval_balanced_accuracy_score': 0.857033639143731, 'eval_AUC': 0.8570336391437311, 'eval_runtime': 20.6781, 'eval_samples_per_second': 11.413, 'eval_steps_per_second': 0.193, 'epoch': 1.0}\n",
      "{'train_runtime': 364.524, 'train_samples_per_second': 2.59, 'train_steps_per_second': 0.041, 'train_loss': 0.6600124994913737, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# apply torch.optim.AdamW instead\n",
    "optimizer = torch.optim.AdamW(classification_model.parameters(), lr=1e-4)\n",
    "trainer.optimizer = optimizer\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# save model\n",
    "trainer.save_model(\"./models/classifiers/best_\"+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b896882d2b43ebb538571d46a1b7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.592046856880188,\n",
       " 'eval_accuracy': 0.8569678159192854,\n",
       " 'eval_precision': 0.8754064674119615,\n",
       " 'eval_recall': 0.8333333333333334,\n",
       " 'eval_f1': 0.8568942986915573,\n",
       " 'eval_balanced_accuracy_score': 0.857033639143731,\n",
       " 'eval_AUC': 0.8570336391437311,\n",
       " 'eval_runtime': 21.1793,\n",
       " 'eval_samples_per_second': 11.143,\n",
       " 'eval_steps_per_second': 0.189,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxicity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
