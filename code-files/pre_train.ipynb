{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 01:27:19.109677: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/isiddig/anaconda3/envs/toxicity/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_cuda.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from smiles_spe_tokenizer import SMILES_SPE_Tokenizer\n",
    "import datasets\n",
    "import torch\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer , DataCollatorWithPadding , AutoTokenizer, AutoConfig , AutoModelForCausalLM\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline, RobertaModel, RobertaTokenizer\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, pipeline\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import evaluate\n",
    "from bertviz_repo.bertviz import head_view\n",
    "from CollatorDatasetTrainer import CustomDataCollator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from CollatorDatasetTrainer import CustomDataCollator , ToxicTrainer\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from utils import split_stratified_into_train_val_test\n",
    "from datasets import load_metric\n",
    "# from transformers.integrations import EarlyStoppingCallback\n",
    "# import balanced accuracy score\n",
    "from sklearn.metrics import balanced_accuracy_score , accuracy_score , precision_score , recall_score , f1_score , roc_auc_score\n",
    "from bert_loves_chemistry.chemberta.utils.molnet_dataloader import load_molnet_dataset, write_molnet_dataset_for_chemprop\n",
    "# torch.cuda.is_available()\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = 'Molecular_language_models.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the tokenizers\n",
    "Spe_tokenizer = SMILES_SPE_Tokenizer(vocab_file='vocabs/spe_vocab/vocab_speSMILESDB_1000.txt', spe_file= 'vocabs/spe_vocab/cleaned_smilesDB_1000.smi', tokenizer_type='SPE')\n",
    "# use white space tokenizer\n",
    "MacFrag_tokenizer = SMILES_SPE_Tokenizer(vocab_file='vocabs/macFrag_vocab/smilesDB_vocab.smi', tokenizer_type='MacFrag')\n",
    "\n",
    "Bpe_model_pt = 'models/tokenizers/bpe_DBsmi_1000.bin'\n",
    "Bpe_vocab_pt = 'vocabs/bpe_vocab/bpe_DBsmi_vocab_1000.txt'\n",
    "Bpe_tokenizer = SMILES_SPE_Tokenizer(vocab_file= Bpe_vocab_pt, \n",
    "                                     tokenizer_path= Bpe_model_pt,\n",
    "                                     tokenizer_type='BPE')\n",
    "\n",
    "morf_model_pt='models/tokenizers/morf_smilesDB_1000.bin' #'models/tokenizers/morf_zinc_1000.bin'\n",
    "morf_vocab_pt = './vocabs/morf_vocab/morf_smilesDB_vocab1000.txt' # './vocabs/morf_vocab/morf_zinc250k_vocab1000.txt'\n",
    "mofressor_tokenizer = SMILES_SPE_Tokenizer(vocab_file=morf_vocab_pt,\n",
    "                                           tokenizer_path=morf_model_pt,\n",
    "                                           tokenizer_type='Morfessor')\n",
    "\n",
    "# choose tokenizer \n",
    "tokenizer = Bpe_tokenizer #MacFrag_tokenizer #mofressor_tokenizer #Bpe_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'S(=O)(=O)', 'C', 'C', 'N', 'C', 'c1ccc(', 'o1', ')', 'c2ccc3c(c2)', 'c(', 'ncn', '3', ')N', 'c4ccc(', 'c(', 'c4', ')', 'Cl)', 'O', 'C', 'c5ccc', 'c(', 'c5', ')', 'F']\n",
      "['CS(=O)(=O)CCNCc1ccc(o1)c2ccc3c(c2) ', ' c(ncn3)Nc4ccc(c(c4)Cl)OCc5cccc(c5)F']\n",
      "['CS(=O)(=O)', 'CCN', 'Cc1ccc(', 'o1)', 'c2ccc3c(c2)', 'c(', 'ncn', '3)N', 'c4ccc(', 'c(', 'c4)C', 'l)', 'OC', 'c5cccc', '(', 'c5)', 'F']\n",
      "['CS(=O)(=O)', 'CCN', 'Cc1ccc(', 'o1)', 'c2ccc3c(c2)', 'c(', 'ncn', '3)', 'N', 'c4cc', 'c(', 'c(', 'c4)', 'Cl)', 'OC', 'c5', 'cccc', '(', 'c5', ')', 'F']\n"
     ]
    }
   ],
   "source": [
    "print(mofressor_tokenizer._tokenize('CS(=O)(=O)CCNCc1ccc(o1)c2ccc3c(c2)c(ncn3)Nc4ccc(c(c4)Cl)OCc5cccc(c5)F'))\n",
    "print(MacFrag_tokenizer._tokenize('CS(=O)(=O)CCNCc1ccc(o1)c2ccc3c(c2) , c(ncn3)Nc4ccc(c(c4)Cl)OCc5cccc(c5)F'))\n",
    "print(Bpe_tokenizer._tokenize('CS(=O)(=O)CCNCc1ccc(o1)c2ccc3c(c2)c(ncn3)Nc4ccc(c(c4)Cl)OCc5cccc(c5)F'))\n",
    "print(Spe_tokenizer._tokenize('CS(=O)(=O)CCNCc1ccc(o1)c2ccc3c(c2)c(ncn3)Nc4ccc(c(c4)Cl)OCc5cccc(c5)F'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (1180, 4) valid shape (296, 4)\n",
      "train dataset shape (1180, 1) valid dataset shape (296, 1)\n",
      "Tokenization and encoding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a62ad96c7074163b4cec0f305d3a082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf4b72f0348425d929c68f924c6c60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf4672d3ccf4cc1af6c9c348d782d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0c8c6d91664e448d28b3487699211d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train dataset: 1180\n",
      "length of val dataset: 296\n"
     ]
    }
   ],
   "source": [
    "# prepare datasets \n",
    "import pandas as pd\n",
    "clin_tox_zinc250 = \"./datasets/cleaned_smilesDB.smi\"\n",
    "clin = \"./datasets/pre_processed/clintox.smi\"\n",
    "zinc  = \"./datasets/zinc_smiles.txt\"\n",
    "# read clin_tox_zinc250 smiles and store in a list\n",
    "with open(clin_tox_zinc250, \"r\") as f:\n",
    "         smilesDB = f.read().strip().split(\"\\n\")\n",
    "    \n",
    "# clintox_frags = './datasets/mac_fragments/clintox_fragments.smi'\n",
    "# tasks, (train_df, valid_df, test_df), transformers = load_molnet_dataset(\"clintox\", tasks_wanted=None, split='random')\n",
    "# tasks, (train_df, valid_df, test_df), transformers = load_molnet_dataset(\"tox21\", tasks_wanted= ['NR-AhR'])\n",
    "clintox_df = pd.read_csv('./datasets/post_processed/clintox_processed.csv' )\n",
    "tox21_df = pd.read_csv('./datasets/post_processed/tox21_processed.csv' )\n",
    "# split imbalanced dataset clintox into train, valid, test\n",
    "train_df, valid_df = train_test_split(clintox_df, test_size=0.2, random_state=42 , stratify=clintox_df['CT_TOX'], shuffle=True)\n",
    "print(\"train shape\" , train_df.shape, \"valid shape\" , valid_df.shape)\n",
    "# Create a dictionary with the text data\n",
    "tr_data_dict = {\"text\": train_df['smiles'].tolist()} \n",
    "val_data_dict = {\"text\": valid_df['smiles'].tolist()}\n",
    "\n",
    "# Create a dataset using the `Dataset` class from `datasets`\n",
    "tr_dataset= Dataset.from_dict(tr_data_dict)\n",
    "val_dataset= Dataset.from_dict(val_data_dict) \n",
    "\n",
    "print(\"train dataset shape\" , tr_dataset.shape, \"valid dataset shape\" , val_dataset.shape)\n",
    "print(\"Tokenization and encoding\")\n",
    "tr_tokenized_dataset = tr_dataset.map(lambda examples: tokenizer(examples[\"text\"], truncation=True,padding=True, max_length=128), batched=True)\n",
    "val_tokenized_dataset = val_dataset.map(lambda examples: tokenizer(examples[\"text\"], truncation=True,padding=True, max_length=128), batched=True)\n",
    "tr_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'text'])\n",
    "val_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'text'])\n",
    "# call add_decoded_tokens to add decoded tokens to the data\n",
    "tr_tokenized_dataset = tr_tokenized_dataset.map(lambda batch: add_decoded_tokens(batch= batch, tokenizer=tokenizer))\n",
    "val_tokenized_dataset = val_tokenized_dataset.map(lambda batch: add_decoded_tokens(batch= batch, tokenizer=tokenizer))\n",
    "print(\"length of train dataset:\", len(tr_tokenized_dataset))\n",
    "print(\"length of val dataset:\", len(val_tokenized_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'C[NH+]1CCCC(C1)CC2c3ccccc3Sc4c2cccc4',\n",
       " 'input_ids': tensor([ 12,  11,  11, 150, 848,  11, 988, 873, 119,  13,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'decoded_tokens': '[CLS] [UNK] [UNK] 1) CC2 [UNK] c4 c2cccc 4 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_tokenized_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = RobertaConfig(\n",
    "    # vocab_size=52_000,\n",
    "    max_position_embeddings=512,\n",
    "    num_attention_heads=6,\n",
    "    num_hidden_layers=3,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "model_name= \"Clintox_Bpe_chemBERTa\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/pre_trained_models/\"+model_name,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    save_steps=10000,\n",
    "    # save_total_limit=2,\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"no\",\n",
    "    seed=42,\n",
    "    # load_best_model_at_end=True, # requires save strategy to be \"none\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tr_tokenized_dataset,\n",
    "    eval_dataset=val_tokenized_dataset,\n",
    "    # prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isiddig/anaconda3/envs/toxicity/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mibrahim-siddig-i-e\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/local/home/isiddig/Desktop/uds/thesis/project-code/wandb/run-20230629_013018-en6nlyis</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ibrahim-siddig-i-e/huggingface/runs/en6nlyis' target=\"_blank\">brisk-snowball-34</a></strong> to <a href='https://wandb.ai/ibrahim-siddig-i-e/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ibrahim-siddig-i-e/huggingface' target=\"_blank\">https://wandb.ai/ibrahim-siddig-i-e/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ibrahim-siddig-i-e/huggingface/runs/en6nlyis' target=\"_blank\">https://wandb.ai/ibrahim-siddig-i-e/huggingface/runs/en6nlyis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b688975a0846c486b208abf8d03c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.2075, 'learning_rate': 4.736842105263158e-05, 'epoch': 0.26}\n",
      "{'loss': 2.9651, 'learning_rate': 4.473684210526316e-05, 'epoch': 0.53}\n",
      "{'loss': 1.9846, 'learning_rate': 4.210526315789474e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0be9763384f45f3a6f861e9aa3294d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7431315183639526, 'eval_runtime': 42.1898, 'eval_samples_per_second': 7.016, 'eval_steps_per_second': 0.119, 'epoch': 1.0}\n",
      "{'loss': 1.7281, 'learning_rate': 3.9473684210526316e-05, 'epoch': 1.05}\n",
      "{'loss': 1.5935, 'learning_rate': 3.6842105263157895e-05, 'epoch': 1.32}\n",
      "{'loss': 1.6496, 'learning_rate': 3.421052631578947e-05, 'epoch': 1.58}\n",
      "{'loss': 1.5431, 'learning_rate': 3.157894736842105e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2a377fb566487c9544d8b04102d51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.311867356300354, 'eval_runtime': 15.9154, 'eval_samples_per_second': 18.598, 'eval_steps_per_second': 0.314, 'epoch': 2.0}\n",
      "{'loss': 1.3419, 'learning_rate': 2.8947368421052634e-05, 'epoch': 2.11}\n",
      "{'loss': 1.2701, 'learning_rate': 2.6315789473684212e-05, 'epoch': 2.37}\n",
      "{'loss': 1.2715, 'learning_rate': 2.368421052631579e-05, 'epoch': 2.63}\n",
      "{'loss': 1.2893, 'learning_rate': 2.105263157894737e-05, 'epoch': 2.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a917e5c66ba243eebfe4852bc9c3a339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1228598356246948, 'eval_runtime': 15.9011, 'eval_samples_per_second': 18.615, 'eval_steps_per_second': 0.314, 'epoch': 3.0}\n",
      "{'loss': 1.1329, 'learning_rate': 1.8421052631578947e-05, 'epoch': 3.16}\n",
      "{'loss': 1.0368, 'learning_rate': 1.5789473684210526e-05, 'epoch': 3.42}\n",
      "{'loss': 1.0954, 'learning_rate': 1.3157894736842106e-05, 'epoch': 3.68}\n",
      "{'loss': 1.1096, 'learning_rate': 1.0526315789473684e-05, 'epoch': 3.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01930507f8a4e1788f72eda1f43a926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9994926452636719, 'eval_runtime': 15.9572, 'eval_samples_per_second': 18.55, 'eval_steps_per_second': 0.313, 'epoch': 4.0}\n",
      "{'loss': 0.8633, 'learning_rate': 7.894736842105263e-06, 'epoch': 4.21}\n",
      "{'loss': 1.1417, 'learning_rate': 5.263157894736842e-06, 'epoch': 4.47}\n",
      "{'loss': 0.9327, 'learning_rate': 2.631578947368421e-06, 'epoch': 4.74}\n",
      "{'loss': 1.004, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cfc8d294444729bed926576ad8bfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1040592193603516, 'eval_runtime': 16.2914, 'eval_samples_per_second': 18.169, 'eval_steps_per_second': 0.307, 'epoch': 5.0}\n",
      "{'train_runtime': 1775.4085, 'train_samples_per_second': 3.323, 'train_steps_per_second': 0.054, 'train_loss': 1.6926688796595524, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5c8c92b83c49a7b281617bc53fb6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0414453744888306,\n",
       " 'eval_runtime': 16.2487,\n",
       " 'eval_samples_per_second': 18.217,\n",
       " 'eval_steps_per_second': 0.308,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./models/pre_trained_models/Best_\"+ model_name)\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxicity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
